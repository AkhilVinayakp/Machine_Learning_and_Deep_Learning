{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97cd4b2-0127-48ea-8237-641c715f88c9",
   "metadata": {},
   "source": [
    "# **LangChain Expression Language (LCEL) - Model and Prompt Templates**\n",
    "\n",
    "<img src=\"images/langchain_model_io.jpg\">\n",
    "\n",
    "LangChain Expression Language (LCEL) makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging. \n",
    "\n",
    "The most basic and common use case is chaining a prompt template and a model together. \n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4402b36a-ced8-4157-a393-32f1a56f0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23aea24-ce81-4540-8b2e-bf4e09fd6a67",
   "metadata": {},
   "source": [
    "## **Models - LLM and ChatModel**\n",
    "- A model can be a **LLM** or a **ChatModel**.\n",
    "- LLMs handle various language operations such as translation, summarization, question answering, and content creation.\n",
    "- Chat Models are customized for conversational usage.\n",
    "- The output of a ChatModel (and therefore, of this chain) is a message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbc9226-091d-4c73-81a4-70e4e873112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 28 states and 8 union territories in India.\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI LLM Model\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a LLM model\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create a prompt\n",
    "prompt = \"How many states are there in India?\"\n",
    "\n",
    "# Pass the prompt to llm\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745ecff-bf27-4e51-8562-1b68c3cc3d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why did the data scientist break up with their computer? \\n\\nBecause it couldn't handle their complex relationship!\" response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 15, 'total_tokens': 36}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = \"Tell me a short joke about Data Science\"\n",
    "\n",
    "print(chat_model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f2a7cc-9776-4952-9a68-572b5ff5dc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning is a subset of artificial intelligence that focuses on developing algorithms and statistical models that allow computer systems to learn from and make decisions or predictions based on data without being explicitly programmed to do so. Machine learning algorithms use statistical techniques to enable computers to identify patterns and relationships within data, which can then be used to make predictions or decisions. Machine learning is used in a wide range of applications, including image and speech recognition, recommendation systems, medical diagnosis, and autonomous vehicles.', response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 12, 'total_tokens': 106}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatModel\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "prompt = \"What is Machine Learning?\"\n",
    "\n",
    "chain = model\n",
    "\n",
    "chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623",
   "metadata": {},
   "source": [
    "## **Prompts - PromptTemplate and ChatPromptTemplate**\n",
    "\n",
    "Input to a model can be a **string** value or **chat message** list.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM.\n",
    "- Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "- A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
    "- LangChain provides tooling to create and work with prompt templates.\n",
    "- LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n",
    "- Typically, language models expect the prompt to either be a string or else a list of chat messages.\n",
    "\n",
    "\n",
    "Prompt templates are used to convert raw user input to a better input to the LLM or ChatModels.  \n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63bb6-dc92-4a62-bd85-7a61989b9321",
   "metadata": {},
   "source": [
    "### **PromptTemplate and from_template()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e2ef49-861a-4249-be5c-d36d06cdb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bb94d5-c45c-4eed-9e74-43cd323cebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4357d6-0223-47ad-8d81-666d6f839958",
   "metadata": {},
   "source": [
    "### **PromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
    "- .format_prompt(): Converts the PromptTempate to `StringPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcb0059-7e24-4e0c-bd13-71fc8b6e6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd2d02b-1c5e-4845-97ab-9bcf8acb3e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptTemplate' object has no attribute 'format_messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m(adjective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunny\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchickens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(prompt))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PromptTemplate' object has no attribute 'format_messages'"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format_messages(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58a1e07-59f4-4ff2-ab0d-2a87023dc6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "text='Tell me a funny joke about chickens.'\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a string i.e. StingPromptValue\n",
    "# PromptValue can be converted to either Strings or Messages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aeec0c1-146a-433e-85e4-cb5ce5aa12e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.')]\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to List of ChatMessages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\").to_messages()\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba8600-1dbe-4006-8c5e-5ab5ae077c28",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate and from_template()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a736db-8c78-439a-8c94-5d6c2b7b4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    \"What is {topic}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f7c21c-93ba-4cfb-ba5b-338835325024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412fc6-a79e-4eb7-a98a-d776930a7837",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
    "- .format_prompt(): Converts the PromptTempate to `ChatPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a49ce6-dc0d-4ad5-88e7-01315c61d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Human: What is machine learning?\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = chat_template.format(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e079d5a5-a54c-4dbe-9e26-329d0c81b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='What is machine learning?')]\n"
     ]
    }
   ],
   "source": [
    "# fomat_messages() return list of chat messages\n",
    "\n",
    "prompt = chat_template.format_messages(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e119b0-2261-4d38-a01c-eff90fb2dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[HumanMessage(content='What is machine learning?')]\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "# PromptValue can be converted to either Strings or Chat Messages\n",
    "\n",
    "prompt = chat_template.format_prompt(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c39e2c-2b73-49ad-9c05-fb778f60796b",
   "metadata": {},
   "source": [
    "## **Few-Shot ChatPromptTemplate and from_messages()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "927ffc3f-c3c2-4ef2-9edf-1c7d77bb8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b093e8-2626-49a1-9e5a-69abaaa2e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is Bob.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: What is your name?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1ae895-54ad-4eaa-a80c-149bb54dbecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages() returns chat messages \n",
    "\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02998b4b-db2a-4c66-a0b6-6b39c5c5132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "\n",
    "chat_template.format_prompt(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f36f2-9f16-4239-9572-a9ca72eec8df",
   "metadata": {},
   "source": [
    "## **Designing ChatPromptTemplate using SystemMessagePromptTemplate, HumanMessagePromptTemplate and AIMessagePromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02925376-d6fb-4055-a254-4d1fb9b9384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "# System Template\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Template\n",
    "ai_prompt_template = AIMessagePromptTemplate.from_template(\"I'm doing well, thanks!\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template_input = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt_template, human_prompt_template, ai_prompt_template, human_prompt_template_input]\n",
    ")\n",
    "\n",
    "chat_template.format_prompt(name=\"Bob\", user_input=\"What is your name?\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbc7e0-98a8-47d8-9fd5-39cd10076fbf",
   "metadata": {},
   "source": [
    "## **LCEL**  \n",
    "`PromptTemplate` and `ChatPromptTemplate` implement the Runnable interface, the basic building block of the **LangChain Expression Language (LCEL)**. This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.\n",
    "\n",
    "**Using `invoke()`:**  \n",
    "`PromptTemplate` accepts a dictionary (of the prompt variables) and returns a `StringPromptValue`.  \n",
    "A `ChatPromptTemplate` accepts a dictionary and returns a `ChatPromptValue`.\n",
    "\n",
    "`PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21c57c5-674a-4f82-a849-28df128e2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')]\n"
     ]
    }
   ],
   "source": [
    "prompt = chat_template.invoke({\"name\": \"Bob\", \"user_input\": \"What is your name?\"})\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "002333c5-0cd0-459f-a2a2-cbc79708301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is Bob.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: What is your name?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37b48fd4-0b54-42dc-bfd8-85ec64b96c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
