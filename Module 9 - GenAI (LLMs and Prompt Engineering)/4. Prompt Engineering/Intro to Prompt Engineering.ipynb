{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812263d8-7996-496a-bf3a-024349f1008d",
   "metadata": {},
   "source": [
    "# **Prompt Engineering**\n",
    "\n",
    "## **Alignment in LLMs and Prompt Engineering**\n",
    "1. Alignment in Language Model refers to how well the model can understand and respond to the input prompts that match the user's expectations. Put another way, an aligned LLM has an objective that matches a human's objective.\n",
    "2. A popular method of aligning language model is through the incorporation of Reinforcement Learning into the training loop.\n",
    "3. **Reinforcement Learning with Human Feedback (RLHF)** is a popular method of aligning pre-trained LLMs that uses human feedback to enhance their performance.\n",
    "4. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**.\n",
    "5. **Prompt Engineering** involves crafting prompts that effectively communicate the task at hand to the LLM, leading to accurate and useful outputs.\n",
    "6. Few Language Models that have been specifically designed and trained to be aligned with instructional prompts are GPT-3, GPT-4, ChatGPT (closed-source model from OpenAI), FLAN-T5 (an open-source model from Google) and Cohere's command series (closed-source).\n",
    "7. Let us now learn few important **Prompt Engineering Techniques** which can help us get the desired and improved performance of language models.\n",
    "\n",
    "## **Prompt Engineering Techniques**\n",
    "\n",
    "### **1. Just Ask**\n",
    "- First and most important rule of Prompt Engineering for instruction aligned language models is to be clear and direct in what you are asking for.\n",
    "- To be even more confident in LLM's response, we can provide a clear indication of the input and output for the task by adding prefixes.\n",
    "- A simple \"just ask\" prompt can also be modified to consist of three elements: A direct instruction, Prefix to denote the input with input phrase and prefix to denote the output with space designated for the LLM to answer.<br>\n",
    "<img style=\"float: left;\" width=\"300\" height=\"300\" src=\"data/images/1_just_ask.JPG\">\n",
    "<img style=\"float: center;\" width=\"300\" height=\"300\" src=\"data/images/2_just_ask_modified_prefixes.JPG\">\n",
    "<br />\n",
    "\n",
    "### **2. Few-shot Learning**\n",
    "<br>\n",
    "\n",
    "<img width=\"400\" height=\"400\" src=\"data/images/4_output_without_few_shots.JPG\">\n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/3_few_shot_learning.JPG\">\n",
    "\n",
    "- Above is the example with \"zero shot\".\n",
    "- For more complex tasks, giving LLM a few example can go a long way in helping an LLM produce accurate and consistent output.\n",
    "- Few-shot learning is a powerful technique that involves providing an LLM with a few examples of a task to help it understand the context and nuances of the problem.\n",
    "- With this technique, we can provide an LLM with an understanding of a task without explicitly providing instructions, making it more intuitive and user-friendly.\n",
    "- On right side you see an example of few shot learning.\n",
    "<br />\n",
    "\n",
    "### **3. Output Structuring**\n",
    "- LLMs can generate text in variety of formats.\n",
    "- We can make an LLM give back structured data formats like JSON as the output.\n",
    "- Using a structured format can help ensure consistency in the output and reduce the risk of errors or inconsistencies.<br>\n",
    "<img style=\"float: left;\" width=\"300\" height=\"300\" src=\"data/images/5_output_structuring_1.JPG\">\n",
    "<img style=\"float: left;\" width=\"300\" height=\"300\" src=\"data/images/6_output_structuring_2.JPG\">\n",
    "<img style=\"float: center;\" width=\"300\" height=\"300\" src=\"data/images/7_output_structuring_3.JPG\">\n",
    "<br /><br />\n",
    "\n",
    "### **4. Prompting Personas**\n",
    "- Personas can be based on specific topics, geners, or even fictional characters, and are designed to elicit specific types of responses from the LLM.\n",
    "- By taking advantage of personas, LLM developers can better control the output of the model and end-users of the system can get a more unique and tailored experience. <br>\n",
    "<img style=\"float: left;\" width=\"300\" height=\"300\" src=\"data/images/8_prompt_personas_1.JPG\">\n",
    "<img style=\"float: left;\" width=\"300\" height=\"300\" src=\"data/images/9_prompt_personas_2.JPG\">\n",
    "<img style=\"float: center;\" width=\"300\" height=\"300\" src=\"data/images/10_prompt_personas_3.JPG\">\n",
    "<br>\n",
    "\n",
    "### **5. Working with Prompts Across Models**\n",
    "- **Remember:** What works for one model may not work for another. Prompts are highly dependent on the architecture and training of the language model. For eg: ChatGPT, GPT-3, GPT-4, T5, Cohere all have different underlying architecutres, pre-training data sources and training approaches.\n",
    "- **ChatGPT:**\n",
    "    - Closed-source model from OpenAI\n",
    "    - Models that are aligned to conversational dialogue like ChatGPT can take in a `system prompt` and multiple `user` and `assistant` prompts.\n",
    "    - System prompt is meant to be a general directive for the conversation and will generally include overarching rules and personas to follow.\n",
    "    - User and Assistant prompts are messages between the user and the LLM respectively.\n",
    "- **Coral:**\n",
    "    - Closed-source model from Cohere\n",
    "    - Need more hand-holding and structuring of prompts to get desired output\n",
    "- **Open-Sourced**\n",
    "    - Eg: GPT-J and FLAN-T5\n",
    "    - Can generate high-quality text output just like their closed-source counterparts.\n",
    "    - Offers greater flexibility and control over prompt engineering, this enables developers to customize prompts and tailor output to specific use cases during fine-tuning.\n",
    "    - GPT-J is a autoregressive language model which is not instruction aligned, so we'd expect thing like few shot prompting to work better than simply asking a direct instruction prompt.\n",
    "    - FLAN-T5 was specifically fine-tuned with instructional prompting in mind so while few-shot will still be on the table, we can also rely on the simplicity of just asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226122e-a9fc-4cc5-a32c-7604ce847128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
