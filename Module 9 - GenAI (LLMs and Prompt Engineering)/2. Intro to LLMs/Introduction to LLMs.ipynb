{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6b77d0-527c-493f-ad64-db06728b0d9e",
   "metadata": {},
   "source": [
    "# **Introduction to LLMs**\n",
    "\n",
    "## **What's Covered**\n",
    "1. An Era Before Transformers\n",
    "2. Attention is all you need\n",
    "3. A little bit about Transformers\n",
    "4. Advantages of Transformers\n",
    "5. Disadvantages of Transformers\n",
    "6. What is Language Modeling?\n",
    "7. What are LLMs?\n",
    "8. Pre-Training, Transfer Learning and Fine-Tuning\n",
    "9. Popular Modern LLMs\n",
    "    - BERT\n",
    "    - GPT\n",
    "    - T5\n",
    "    - Domain Specific LLMs\n",
    "10. Prompt Engineering\n",
    "11. Applications\n",
    "12. Quick Summary\n",
    "13. What Next? How to use LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c515617-57f8-42f5-b1b1-13406bb8e8e3",
   "metadata": {},
   "source": [
    "## **An Era Before Transformers**\n",
    "\n",
    "1. **2013 and before:** Various Neural Network Architectures like ANN, CNN and RNN became very popular. They use to work well for tabular data, image data and sequential data like text respectively.\n",
    "2. **[(2014) Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)** paper introduced the concept of **Encoder-Decoder Architecture** to solve a seq2seq task, like machine translation.\n",
    "    - The paper introduces Seq2Seq models, which are neural network architectures designed for mapping input sequences to output sequences. Unlike traditional models that rely on fixed-length input-output mappings, Seq2Seq models can handle variable-length sequences, making them suitable for tasks such as machine translation, summarization, and question answering.\n",
    "    - The core of the Seq2Seq model is the encoder-decoder architecture. The encoder processes the input sequence while maintaining the hidden state and generates a fixed-length representation, often referred to as a context vector. This context vector encapsulates the representation of the whole sentence.\n",
    "    - The decoder then uses this representation to generate the output sequence one token at a time.\n",
    "    - Both encoder and decoder used RNN/LSTM cells due to their ability to capture sequential dependencies.\n",
    "    - This architecture used to work well with smaller sentence.\n",
    "    - **The Problem:** While it could handle variable-length input and output sequences, it used to rely on generating a single fixed-length context vector for the entire input sequence, which can lead to information loss, especially for longer sequences.\n",
    "3. **[(2015) Neural Machine Translation by Joint Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)** paper introduced the concept of **Attention Mechanism** to solve the above problem.\n",
    "    - Unlike traditional NMT models that encode the entire source sentence into a fixed-length context vector, the attention mechanism allows the model to focus on different parts of the source sentence dynamically while generating the translation.\n",
    "    - Attention Mechanism also addressed the problem of learning alignment between input and output sequences, enables the model to weigh the importance of each word in the source sentence differently during translation. By dynamically adjusting the attention weights, the model can focus more on relevant words and ignore irrelevant ones, leading to more accurate translations.\n",
    "    - At each timestamp of the decoder, the dynamically calculated context vector indicates which timestamps of the encoder sequence are expected to have the most influence on the current decoding step of the decoder.\n",
    "    - In simple terms, context vector will be the weighted sum of encoders hidden state. And these weights are called as **attention weights**.\n",
    "    - The attention mechanism has improved, the quality of translation on long input sentences. But it was not able to solve a huge fundamental flaw i.e. sequential training.\n",
    "    - **The Problem:** Since the architecture relies on LSTM units, a notable challenge arises due to the sequential nature of training. Specifically, only one token can be processed at a time as input to the encoder, leading to slow training times. Consequently, it becomes impractical to train the model efficiently with large datasets. This limitation inhibits the application of techniques like transfer learning, which typically involve leveraging pretrained models on large datasets to improve performance on new tasks. Additionally, fine-tuning, which involves further training pretrained models on task-specific data, is also hindered by the slow training process in this architecture.\n",
    "    - Now because of the above problem, for any task which we are suppose to solve, we have to train the model from scratch. And it takes a huge amount of time, efforts and data.\n",
    "    - **Transfer Learning:** Transfer learning involves leveraging knowledge gained from solving one problem and applying it to a different, but related, problem.\n",
    "    - **Fine-Tuning:** Fine-tuning, on the other hand, refers to the process of taking a pretrained model and further training it on task-specific data to adapt it to a particular problem or domain. This typically involves adjusting the parameters of the pretrained model to better suit the new task while retaining the knowledge learned from the original training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00ce99-138d-4b96-ab80-f24928eb4029",
   "metadata": {},
   "source": [
    "## **Attention is all you need**\n",
    "\n",
    "**[(2017) Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)** paper introduced by Google which solves the sequential training problem of earlier architecture by removing the need of RNN cells completely.\n",
    "1. It has the encoder-decoder architecture.\n",
    "2. It relies solely on self-attention mechanisms and feed-forward neural networks.\n",
    "3. Recall that **Attention** is a mechanism that assigns different weights to different parts of the input allowing the model to prioritize and emphasize the most important information while performing tasks like translation or summarization. Attention allows a model to focus on different parts of the input dynamically, leading to improved performance.\n",
    "4. **Self-Attention Mechanism:** The key innovation of the Transformer is the self-attention mechanism, which allows each word in the input sequence to attend to all other words in the sequence. This enables capturing global dependencies and alleviates the need for recurrent connections.\n",
    "5. **Positional Encoding:** To retain positional information of words in the input sequence without using recurrence, the model introduces positional encodings. These encodings are added to the input embeddings to provide information about the position of each word in the sequence.\n",
    "6. **Multi-Head Attention:** The Transformer employs multi-head attention mechanisms, where attention is computed multiple times in parallel with different learned linear projections. This allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture diverse patterns.\n",
    "7. **Parallelization and Scalability:** By relying on self-attention mechanisms and feed-forward layers, the Transformer architecture facilitates parallelization of computation across different parts of the input sequence. This results in faster training times and better scalability compared to traditional recurrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b6f3b-e318-4607-baea-878ecd6fef78",
   "metadata": {},
   "source": [
    "## **A little bit about Transformers**\n",
    "\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/transformer.jpeg\">\n",
    "\n",
    "1. Introduced by Google in the year 2017\n",
    "2. Transformer is a Sequence to Sequence Model which was proposed initially to solve the task of Machine Translation\n",
    "3. Has two main components: Encoder-Decoder and Attention Mechanism\n",
    "4. An **encoder** which is tasked with taking in raw text, splitting them up into its core components, convert them into vectors and using **self-attention** to understand the context of the text.\n",
    "5. A **decoder** excels at generating text by using a modified type of attention (i.e. **cross attention**) to predict the next best token.\n",
    "6. Transformers revolutionized NLP by enabling highly scalable training. By leveraging parallel computation and efficient self-attention mechanisms, the Transformer architecture allows for training on massive datasets with unprecedented efficiency. This scalability laid the foundation for the concept of **Transfer Learning** in NLP. Subsequent models such as BERT, GPT, and T5 were developed, leveraging pre-trained Transformer-based architectures that could be easily **fine-tuned** for a wide range of NLP tasks, further advancing the field of natural language processing.\n",
    "7. Transformers are **trained** to solve a specific NLP task called as **Language Modeling**.\n",
    "8. **Why not RNNs? -** RNN units can become a bottleneck due to sequential training. Due to parallel training capabilities and self attention mechanism of transformer, it allows each word to \"attend to\" all the other words in the sequence which enables it to capture long-term dependencies and contextual relationships between words at scale. The goal is to understand each word as it relates to the other tokens in the input text.\n",
    "9. **Limitation:** Transformers are still limited to an input context window (i.e. maximum length of text it can process at any given moment)\n",
    "10. Timeline\n",
    "    - Till 2013 - RNN/LSTMs/GRU\n",
    "    - 2014 - Seq2seq tasks using Encoder-Decoder architecture\n",
    "    - 2015 - Attention Mechanism\n",
    "    - 2017 - Transformers\n",
    "    - 2018 - BERT by Google / GPT by OpenAI\n",
    "    - 2019 - T5 by Google\n",
    "    - 2020 - Stable Diffusion / GPT3\n",
    "    - 2021 - DALL-E / Github Copilot\n",
    "    - 2022 - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e00c8e-37b5-4f8a-9122-d948b9dd5bee",
   "metadata": {},
   "source": [
    "## **Advantages of Transformers**\n",
    "1. Parallel Training and Scalable\n",
    "2. Transfer Learning\n",
    "3. Multimodal Input and Output\n",
    "4. Flexible Architecture: Encoder only transformer models like BERT, Decoder only transformer like GPT and Encode-Decoder based model like T5.\n",
    "5. Ecosystem: HuggingFace, OpenAI, Cohere, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b0961-1a21-437f-914c-5ae1e5991638",
   "metadata": {},
   "source": [
    "## **Disadvantages of Transformers**\n",
    "1. Needs high computational resources like space and GPUs\n",
    "2. Huge amount of Data is required to train a model using transformers\n",
    "3. Overfitting\n",
    "4. Energy Consumptions\n",
    "5. Interpretation\n",
    "6. Biasness due to data and Ethical Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9ef33-5a42-4a32-8c70-d9016a509812",
   "metadata": {},
   "source": [
    "## **What is Language Modeling?**\n",
    "1. Language Modeling involves creation of statistical/deep learning models for predicting the likelyhood of a sequence of tokens in a specified vocabulary.\n",
    "2. Two types of Language Modeling Tasks are:  \n",
    "    a. Autoencoding Task  \n",
    "    b. Autoregressive Task  \n",
    "3. **Autoregressive Language Models** are trained to predict the next token in a sentence, based on the previous tokens in the phrase. These models correspond to the **decoder** part of the transformer model. A mask is applied on the full sentence so that the attention head can only see the tokens that came before. These models are ideal for text generatation. For eg: **GPT**\n",
    "4. **Autoencoding Language Models** are trained to reconstruct the original sentence from a corrupted version of the input. These models correspond to the **encoder** part of the transformer model. Full input is passed. No mask is applied. Autoencoding models create a bidirectional representation of the whole sentence. They can be fine-tuned for a variety of tasks, but their main application is sentence classification or token classification. For eg: **BERT**\n",
    "5. **Combination of autoregressive and autoencoding language models** are more versatile and flexible in generating text. It has been shown that the combination models can generate more diverse and creative text in different context compared to pure decode-based autoregressive models due to their ability to capture additional context using the encoder. For eg: **T5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904d47f-152a-48c9-9911-5e28a2add988",
   "metadata": {},
   "source": [
    "## **What are LLMs?**\n",
    "1. Usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "2. Designed to understand and generate human language, code, and much more.\n",
    "3. Highly parallelized and scalable.\n",
    "4. Example: BERT, GPT and T5\n",
    "5. Techniques like: Stop word removal, stemming, and truncation are not used nor are they necessary for LLMs. LLMs are designed to handle the inherent complexity and variability of human language, including the use of stop words and variations in word forms like tenses and misspellings.\n",
    "6. Every LLM on the market has been **pre-trained** on a large corpus of the text data and on a specific language modeling related tasks.\n",
    "7. **Remember:** How an LLM is **pre-trained** and **fine-tuned** makes all the difference.\n",
    "8. **How to decide whether to train our own embeddings or use pre-trained embeddings?** - A good rule of thumb is to compute the vocabulary overlap. If the overlap between the vocabulary of our custom domain and that of pre-trained word embeddings is significant, pre-trained word embeddings tends to give good results.\n",
    "9. **One more important factor to consider while deploying models with embeddings-based feature extraction approach:** - Remember that learned or pre-trained embedding models have to be stored and loaded into memory while using these approaches. If the model itself is bulky, we need to factor this into our deployment needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e6806-74e3-4041-8aa4-a20f8df2b0a9",
   "metadata": {},
   "source": [
    "## **Pre-Training, Transfer Learning and Fine-Tuning**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/transfer_learning.jpeg\">\n",
    "\n",
    "1. **Pre-training** of an LLM happens on a large corpus of text data and on a specific language modeling related task. During this phase LLM tries to learn and understand general language and relationships between words.\n",
    "2. **Transfer Learning** is a technique used in machine learning to leverage the knowledge gained from one task to improve performance on another related task. Understand that pre-trained model has already learned a lot of information about the language and the relationships between words, and this information can be used as a starting point to improve performance on a new task.  \n",
    "    **a.** Transfer Learning for LLMs involves taking an LLM that has been pre-trained on one corpus of text data and then fine-tuning it for a specific downstream task, such as text classification or text generation, by updating the model's parameter with task-specific data.  \n",
    "    **b.** Transfer Learning allows LLMs to be **fine-tuned** for specific tasks with much smaller amounts of task-specific data than it would require if the model were trained from scratch. This greatly reduces the amount of time and resources required to train LLMs.  \n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/fine_tuning_loop.jpeg\">\n",
    "3. **Fine-tuning** involves training the LLM on a smaller, task-specific dataset to adjust its parameters for the specific task at hand. The basic fine-tuning loop is more or less same.  \n",
    "    **a.** Define a model you want to fine-tune as well as fine-tuning parameters (eg: learning rate)  \n",
    "    **b.** Aggregate some training data.  \n",
    "    **c.** Compute loss and gradients.  \n",
    "    **d.** Update the model via backpropogation.  \n",
    "4. The Transformers package from Hugging Face provides a neat and clean interface for training and fine-tuning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588172b-9c70-4c0f-9732-c5d00b22967f",
   "metadata": {},
   "source": [
    "## **Popular Modern LLMs**\n",
    "\n",
    "#### **1. BERT (Bidirectional Encoder Representation from Transformers)**\n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_oov.jpeg\">\n",
    "\n",
    "1. By Google - Autoencoding Language Model\n",
    "2. Individual NLP tasks have traditionally been solved by individual models created for each specific task. That is, until— BERT!\n",
    "3. Tasks - BERT can solve 11+ NLP tasks such as sentiment analysis, named entity recognition,  etc...\n",
    "4. Pretrained on:  \n",
    "    **a.** English Wikipedia - At the time 2.5 billion words  \n",
    "    **b.** Book Corpus - 800 million words  \n",
    "5. BERT's tokenizer handles OOV tokens (out of vocabulary / previously unknown) by breaking them up into smaller chunks of known tokens.\n",
    "6. Trained on two language modeling specific tasks:  \n",
    "    **a.** **Masked Language Modeling (MLM) aka Autoencoding Task** - Helps BERT recognize token interaction within the sentence.    \n",
    "    **b.** **Next Sentence Prediction (NSP) Task** - Helps BERT to understand how tokens interact with each other between sentences.  \n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_language_model_task.jpeg\">\n",
    "7. BERT uses three layer of token embedding for a given piece of text: Token Embedding, Segment Embedding and Position Embedding.\n",
    "8. BERT uses the encoder of transformer and ignores the decoder to become exceedingly good at processing/understanding massive amounts of text very quickly relative to other slower LLMs that focus on generating text one token at a time.\n",
    "9. BERT itself doesn't classify text or summarize documents but it is often used as a pre-trained model for downstream NLP tasks. \n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_classification.jpeg\">\n",
    "10. 1 year later RoBERTa by Facebook AI shown to not require NSP task. It matched and even beat the original BERT model's performance in many areas.\n",
    "11. Reference: [Click here to read more](https://huggingface.co/blog/bert-101)\n",
    "12. BERT Implementation: [Click here to learn how to use BERT](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)\n",
    "\n",
    "#### **2. GPT (Generative Pre-Trained Transformer)**\n",
    "\n",
    "1. By OpenAI - Autoregressive Language Model.\n",
    "2. Pretrained on: Proprietary Data (Data for which the rights of ownership are restricted so that the ability to freely distribute the is limited)\n",
    "3. Autoregressive Language Model that uses attention to predict the next token in a sequence based on the previous tokens.\n",
    "4. GPT relies on the decoder portion of the Transformer and ignores the encoder to become exceptionally good at generating text one token at a time.\n",
    "\n",
    "#### **3. T5 (Text to Text Transfer Transformer)**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/t5.jpeg\">\n",
    "\n",
    "1. By Google - Combination of Autoencoder and Autoregressor Language Model.\n",
    "2. Tasks: T5 can solve tasks such as summarization, translation, Q&A, and text classification\n",
    "3. T5 uses both encoder and decoder of the Transformer to become highly versatile in both processing and generating text.\n",
    "4. T5 based models can generate wide range of NLP tasks, from text classification to generation.\n",
    "\n",
    "#### **4. Domain Specific LLMs**\n",
    "\n",
    "1. BioGPT - Trained on large scale biomedical literature (more than 2 million articles). Developed by the AI healthcare company, Owkin, in collaboration with Hugging Face.\n",
    "2. SciBERT\n",
    "3. BlueBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65c3cf-2acf-42d1-9307-17b35f38ea07",
   "metadata": {},
   "source": [
    "## **Prompt Engineering**\n",
    "1. Popular LLMs: GPT-3, GPT-4, ChatGPT, Coral, GPT-J, FLAN-T5, etc...\n",
    "2. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**.\n",
    "3. **Prompt Engineering** involves crafting prompts that effectively communicate the task at hand to the LLM, leading to accurate and useful outputs.\n",
    "4. Few Language Models that have been specifically designed and trained to be aligned with instructional prompts are GPT-3, GPT-4, ChatGPT (closed-source model from OpenAI), FLAN-T5 (an open-source model from Google) and Cohere's command series (closed-source)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0be2c2-ff1d-47f6-94c0-563758d420d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Applications:**\n",
    "#### **1. Medical Domain**\n",
    "1. Electronic Medical Record (EMR) Processing\n",
    "2. Clinical Trial Matching\n",
    "3. Drug Discovery\n",
    "\n",
    "#### **2. Finance**\n",
    "1. Fraud Detection\n",
    "2. Sentiment Analysis of Financial News\n",
    "3. Trading Strategies\n",
    "4. Customer Service Automation via Chatbots and Virtual Assistants\n",
    "\n",
    "#### **3. And many more**\n",
    "1. Text Classification\n",
    "2. Text Summarization\n",
    "3. Chatbots\n",
    "4. Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded9146-c262-428f-9042-23f463695a03",
   "metadata": {},
   "source": [
    "## **Quick Summary**\n",
    "1. What really sets the Transformers appart from other deep learning architectures is its ability to capture long-term dependencies and relationships between tokens using attention mechanism.\n",
    "2. Attention is the crucial component of Transformer.\n",
    "3. Factor behind transformer's effectiveness as a language model is it is highly parallelizable, allowing for faster training and efficient processing of text.\n",
    "4. LLMs are usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "5. Designed to understand and generate human language, code, and much more.\n",
    "6. LLMs are pre-trained on large corpus and fine-tuned on smaller datasets for specific tasks.\n",
    "7. Popular LLMs: GPT-3, GPT-4, ChatGPT, Coral, GPT-J, FLAN-T5, etc...\n",
    "8. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95ec3b-ea35-47e2-92e9-a8c845c05b1e",
   "metadata": {},
   "source": [
    "## **What Next? How to use LLMs?**\n",
    "\n",
    "Given a business problem, ask this to yourself:\n",
    "1. What NLP task does it map to?\n",
    "    - Text Classification\n",
    "    - Token Classification\n",
    "    - Text Generation\n",
    "    - Fill-Mask\n",
    "    - Conversational\n",
    "    - Sentence Similarity\n",
    "    - Question Answer\n",
    "    - Summarization\n",
    "    - Table Q&A\n",
    "    - Translation\n",
    "    - Zero-Shot Classification\n",
    "2. Given the task, what model(s) work for that task?\n",
    "\n",
    "**Example:**  \n",
    "> **Business Problem:** Generate a news feed for an app so that users can scroll through  \n",
    "> **Mapping to a NLP task:** Given news article, a standard NLP task is to summarize  \n",
    "\n",
    "Now before we get into how to solve problems like above, a quick note on NLP ecosystem:\n",
    "\n",
    "| Popular Tools | Utility |\n",
    "| :---: | :---: |\n",
    "| **Hugging Face Transformers** | Pre-trained models and Pipelines |\n",
    "| **NLTK** | Classical NLP + corpora |\n",
    "| **SpaCy** | Production grade NLP, especially NER |\n",
    "| **Gensim** | Classical NLP + Word2Vec |\n",
    "| **OpenAI** | ChatGPT, Whisper |\n",
    "| **Spark NLP** | Scale-out, production-grade NLP |\n",
    "| **LangChain** | LLM Workflows |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
