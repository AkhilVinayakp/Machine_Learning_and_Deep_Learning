{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6b77d0-527c-493f-ad64-db06728b0d9e",
   "metadata": {},
   "source": [
    "# **Introduction to LLMs**\n",
    "\n",
    "## **What's Covered**\n",
    "1. A little bit about Transformers\n",
    "2. Attention is all you need\n",
    "3. What is Language Modeling?\n",
    "4. What are LLMs?\n",
    "5. Pre-Training, Transfer Learning and Fine-Tuning\n",
    "6. Popular Modern LLMs\n",
    "    - BERT\n",
    "    - GPT\n",
    "    - T5\n",
    "    - Domain Specific LLMs\n",
    "7. Prompt Engineering\n",
    "8. Applications\n",
    "9. Quick Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686df7bd-ff4d-4a6e-ad8d-c6ad674dc567",
   "metadata": {},
   "source": [
    "## **A little bit about Transformers:**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/transformer.jpeg\">\n",
    "\n",
    "1. Sequence to Sequence Model.\n",
    "2. Has two main components: Encoder and Decoder\n",
    "3. An **encoder** which is tasked with taking in raw text, splitting them up into its core components, convert them into vectors and using **self-attention** to understand the context of the text.\n",
    "4. A **decoder** excels at generating text by using a modified type of attention (i.e. **cross attention**) to predict the next best token.\n",
    "5. Transformers are **trained** to solve a specific NLP task called as **Language Modeling**.\n",
    "6. **Why not RNNs? -** Transformer's self attention mechanism allows each word to \"attend to\" all other words in the sequence which enables it to capture long-term dependencies and contextual relationships between words. The goal is to understand each word as it relates to the other tokens in the input text.\n",
    "7. **Limitation:** Transformers are still limited to an input context window (i.e. maximum length og text it can process at any given moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00ce99-138d-4b96-ab80-f24928eb4029",
   "metadata": {},
   "source": [
    "## **Attention is all you need**\n",
    "1. **Attention** is a mechanism that assigns different weights to different parts of the input allowing the model to prioritize and emphasize the most important information while performing tasks like translation or summarization.\n",
    "2. Attention allows a model to focus on different parts of the input dynamically, leading to improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9ef33-5a42-4a32-8c70-d9016a509812",
   "metadata": {},
   "source": [
    "## **What is Language Modeling?**\n",
    "1. Language Modeling involves creation of statistical/deep learning models for predicting the likelyhood of a sequence of tokens in a specified vocabulary.\n",
    "2. Two types of Language Modeling Tasks are:  \n",
    "    a. Autoencoding Task  \n",
    "    b. Autoregressive Task  \n",
    "3. **Autoregressive Language Models** are trained to predict the next token in a sentence, based on the previous tokens in the phrase. These models correspond to the **decoder** part of the transformer model. A mask is applied on the full sentence so that the attention head can only see the tokens that came before. These models are ideal for text generatation. For eg: **GPT**\n",
    "4. **Autoencoding Language Models** are trained to reconstruct the original sentence from a corrupted version of the input. These models correspond to the **encoder** part of the transformer model. Full input is passed. No mask is applied. Autoencoding models create a bidirectional representation of the whole sentence. They can be fine-tuned for a variety of tasks, but their main application is sentence classification or token classification. For eg: **BERT**\n",
    "5. **Combination of autoregressive and autoencoding language models** are more versatile and flexible in generating text. It has been shown that the combination models can generate more diverse and creative text in different context compared to pure decode-based autoregressive models due to their ability to capture additional context using the encoder. For eg: **T5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904d47f-152a-48c9-9911-5e28a2add988",
   "metadata": {},
   "source": [
    "## **What are LLMs?**\n",
    "1. Usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "2. Designed to understand and generate human language, code, and much more.\n",
    "3. Highly parallelized and scalable.\n",
    "4. Example: BERT, GPT and T5\n",
    "5. Techniques like: Stop word removal, stemming, and truncation are not used nor are they necessary for LLMs. LLMs are designed to handle the inherent complexity and variability of human language, including the use of stop words and variations in word forms like tenses and misspellings.\n",
    "6. Every LLM on the market has been **pre-trained** on a large corpus of the text data and on a specific language modeling related tasks.\n",
    "7. **Remember:** How an LLM is **pre-trained** and **fine-tuned** makes all the difference.\n",
    "8. **How to decide whether to train our own embeddings or use pre-trained embeddings?** - A good rule of thumb is to compute the vocabulary overlap. If the overlap between the vocabulary of our custom domain and that of pre-trained word embeddings is significant, pre-trained word embeddings tends to give good results.\n",
    "9. **One more important factor to consider while deploying models with embeddings-based feature extraction approach:** - Remember that learned or pre-trained embedding models have to be stored and loaded into memory while using these approaches. If the model itself is bulky, we need to factor this into our deployment needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e6806-74e3-4041-8aa4-a20f8df2b0a9",
   "metadata": {},
   "source": [
    "## **Pre-Training, Transfer Learning and Fine-Tuning**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/transfer_learning.jpeg\">\n",
    "\n",
    "1. **Pre-training** of an LLM happens on a large corpus of text data and on a specific language modeling related task. During this phase LLM tries to learn and understand general language and relationships between words.\n",
    "2. **Transfer Learning** is a technique used in machine learning to leverage the knowledge gained from one task to improve performance on another related task. Understand that pre-trained model has already learned a lot of information about the language and the relationships between words, and this information can be used as a starting point to improve performance on a new task.  \n",
    "    **a.** Transfer Learning for LLMs involves taking an LLM that has been pre-trained on one corpus of text data and then fine-tuning it for a specific downstream task, such as text classification or text generation, by updating the model's parameter with task-specific data.  \n",
    "    **b.** Transfer Learning allows LLMs to be **fine-tuned** for specific tasks with much smaller amounts of task-specific data than it would require if the model were trained from scratch. This greatly reduces the amount of time and resources required to train LLMs.  \n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/fine_tuning_loop.jpeg\">\n",
    "3. **Fine-tuning** involves training the LLM on a smaller, task-specific dataset to adjust its parameters for the specific task at hand. The basic fine-tuning loop is more or less same.  \n",
    "    **a.** Define a model you want to fine-tune as well as fine-tuning parameters (eg: learning rate)  \n",
    "    **b.** Aggregate some training data.  \n",
    "    **c.** Compute loss and gradients.  \n",
    "    **d.** Update the model via backpropogation.  \n",
    "4. The Transformers package from Hugging Face provides a neat and clean interface for training and fine-tuning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588172b-9c70-4c0f-9732-c5d00b22967f",
   "metadata": {},
   "source": [
    "## **Popular Modern LLMs**\n",
    "\n",
    "#### **1. BERT (Bidirectional Encoder Representation from Transformers)**\n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_oov.jpeg\">\n",
    "\n",
    "1. By Google - Autoencoding Language Model\n",
    "2. Individual NLP tasks have traditionally been solved by individual models created for each specific task. That is, untilâ€” BERT!\n",
    "3. Tasks - BERT can solve 11+ NLP tasks such as sentiment analysis, named entity recognition,  etc...\n",
    "4. Pretrained on:  \n",
    "    **a.** English Wikipedia - At the time 2.5 billion words  \n",
    "    **b.** Book Corpus - 800 million words  \n",
    "5. BERT's tokenizer handles OOV tokens (out of vocabulary / previously unknown) by breaking them up into smaller chunks of known tokens.\n",
    "6. Trained on two language modeling specific tasks:  \n",
    "    **a.** **Masked Language Modeling (MLM) aka Autoencoding Task** - Helps BERT recognize token interaction within the sentence.    \n",
    "    **b.** **Next Sentence Prediction (NSP) Task** - Helps BERT to understand how tokens interact with each other between sentences.  \n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_language_model_task.jpeg\">\n",
    "7. BERT uses three layer of token embedding for a given piece of text: Token Embedding, Segment Embedding and Position Embedding.\n",
    "8. BERT uses the encoder of transformer and ignores the decoder to become exceedingly good at processing/understanding massive amounts of text very quickly relative to other slower LLMs that focus on generating text one token at a time.\n",
    "9. BERT itself doesn't classify text or summarize documents but it is often used as a pre-trained model for downstream NLP tasks. \n",
    "<img style=\"float: right;\" width=\"300\" height=\"300\" src=\"data/images/bert_classification.jpeg\">\n",
    "10. 1 year later RoBERTa by Facebook AI shown to not require NSP task. It matched and even beat the original BERT model's performance in many areas.\n",
    "11. Reference: [Click here to read more](https://huggingface.co/blog/bert-101)\n",
    "12. BERT Implementation: [Click here to learn how to use BERT](https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)\n",
    "\n",
    "#### **2. GPT (Generative Pre-Trained Transformer)**\n",
    "\n",
    "1. By OpenAI - Autoregressive Language Model.\n",
    "2. Pretrained on: Proprietary Data (Data for which the rights of ownership are restricted so that the ability to freely distribute the is limited)\n",
    "3. Autoregressive Language Model that uses attention to predict the next token in a sequence based on the previous tokens.\n",
    "4. GPT relies on the decoder portion of the Transformer and ignores the encoder to become exceptionally good at generating text one token at a time.\n",
    "\n",
    "#### **3. T5 (Text to Text Transfer Transformer)**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/t5.jpeg\">\n",
    "\n",
    "1. By Google - Combination of Autoencoder and Autoregressor Language Model.\n",
    "2. Tasks: T5 can solve tasks such as summarization, translation, Q&A, and text classification\n",
    "3. T5 uses both encoder and decoder of the Transformer to become highly versatile in both processing and generating text.\n",
    "4. T5 based models can generate wide range of NLP tasks, from text classification to generation.\n",
    "\n",
    "#### **4. Domain Specific LLMs**\n",
    "\n",
    "1. BioGPT - Trained on large scale biomedical literature (more than 2 million articles). Developed by the AI healthcare company, Owkin, in collaboration with Hugging Face.\n",
    "2. SciBERT\n",
    "3. BlueBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65c3cf-2acf-42d1-9307-17b35f38ea07",
   "metadata": {},
   "source": [
    "## **Prompt Engineering**\n",
    "1. Popular LLMs: GPT-3, GPT-4, ChatGPT, Coral, GPT-J, FLAN-T5, etc...\n",
    "2. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**.\n",
    "3. **Prompt Engineering** involves crafting prompts that effectively communicate the task at hand to the LLM, leading to accurate and useful outputs.\n",
    "4. Few Language Models that have been specifically designed and trained to be aligned with instructional prompts are GPT-3, GPT-4, ChatGPT (closed-source model from OpenAI), FLAN-T5 (an open-source model from Google) and Cohere's command series (closed-source)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0be2c2-ff1d-47f6-94c0-563758d420d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Applications:**\n",
    "#### **1. Medical Domain**\n",
    "1. Electronic Medical Record (EMR) Processing\n",
    "2. Clinical Trial Matching\n",
    "3. Drug Discovery\n",
    "\n",
    "#### **2. Finance**\n",
    "1. Fraud Detection\n",
    "2. Sentiment Analysis of Financial News\n",
    "3. Trading Strategies\n",
    "4. Customer Service Automation via Chatbots and Virtual Assistants\n",
    "\n",
    "#### **3. And many more**\n",
    "1. Text Classification\n",
    "2. Text Summarization\n",
    "3. Chatbots\n",
    "4. Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded9146-c262-428f-9042-23f463695a03",
   "metadata": {},
   "source": [
    "## **Quick Summary**\n",
    "1. What really sets the Transformers appart from other deep learning architectures is its ability to capture long-term dependencies and relationships between tokens using attention mechanism.\n",
    "2. Attention is the crucial component of Transformer.\n",
    "3. Factor behind transformer's effectiveness as a language model is it is highly parallelizable, allowing for faster training and efficient processing of text.\n",
    "4. LLMs are usually derived from Transformer architecture (but nor necesserily) by training on large amount of text data.\n",
    "5. Designed to understand and generate human language, code, and much more.\n",
    "6. LLMs are pre-trained on large corpus and fine-tuned on smaller datasets for specific tasks.\n",
    "7. Popular LLMs: GPT-3, GPT-4, ChatGPT, Coral, GPT-J, FLAN-T5, etc...\n",
    "8. If you are wondering what is the best way to talk to ChatGPT and GPT-4 to get optimal results, we will cover that under **Prompt Engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95ec3b-ea35-47e2-92e9-a8c845c05b1e",
   "metadata": {},
   "source": [
    "## **What Next? How to use LLMs?**\n",
    "\n",
    "Given a business problem, ask this to yourself:\n",
    "1. What NLP task does it map to?\n",
    "    - Text Classification\n",
    "    - Token Classification\n",
    "    - Text Generation\n",
    "    - Fill-Mask\n",
    "    - Conversational\n",
    "    - Sentence Similarity\n",
    "    - Question Answer\n",
    "    - Summarization\n",
    "    - Table Q&A\n",
    "    - Translation\n",
    "    - Zero-Shot Classification\n",
    "2. Given the task, what model(s) work for that task?\n",
    "\n",
    "**Example:**\n",
    "> **Business Problem:** Generate a news feed for an app so that users can scroll through\n",
    "> **Mapping to a NLP task:** Given news article, a standard NLP task is to summarize\n",
    "\n",
    "Now before we get into how to solve problems like above, a quick note on NLP ecosystem:\n",
    "\n",
    "| Popular Tools | Utility |\n",
    "| :---: | :---: |\n",
    "| **Hugging Face Transformers** | Pre-trained models and Pipelines |\n",
    "| **NLTK** | Classical NLP + corpora |\n",
    "| **SpaCy** | Production grade NLP, especially NER |\n",
    "| **Gensim** | Classical NLP + Word2Vec |\n",
    "| **OpenAI** | ChatGPT, Whisper |\n",
    "| **Spark NLP** | Scale-out, production-grade NLP |\n",
    "| **LangChain** | LLM Workflows |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbe827-1705-48c0-8b0e-c17a2cee53a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
