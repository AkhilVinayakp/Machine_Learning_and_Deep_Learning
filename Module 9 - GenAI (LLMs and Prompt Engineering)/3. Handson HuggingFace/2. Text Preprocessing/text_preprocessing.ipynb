{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c823e87c-bec7-41b8-bd7e-f25bec05b162",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "Reference:  \n",
    "https://huggingface.co/docs/transformers/preprocessing  \n",
    "https://huggingface.co/docs/transformers/tokenizer_summary\n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you‚Äôll learn that for:\n",
    "- Text, use a `Tokenizer` to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.\n",
    "- Speech and audio, use a `Feature extractor` to extract sequential features from audio waveforms and convert them into tensors.\n",
    "- Image inputs use a `ImageProcessor` to convert images into tensors.\n",
    "- Multimodal inputs, use a `Processor` to combine a tokenizer and a feature extractor or image processor.\n",
    "\n",
    "**Note: `AutoProcessor` always works and automatically chooses the correct class for the model you‚Äôre using, whether you‚Äôre using a tokenizer, image processor, feature extractor or processor.**\n",
    "\n",
    "<img width=\"800\" height=\"500\" src=\"data/images/hugging_face_transformers_pipeline.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9cbc2-c6be-478c-9afd-e9a724b66d0d",
   "metadata": {},
   "source": [
    "## **AutoTokenizer**\n",
    "\n",
    "A tokenizer takes text as input and outputs numbers the associated model can make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd6c504-cb8d-4280-9ea2-02fb75ef1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']\n",
      "\n",
      "Tokens Id: [2292, 1005, 1055, 3046, 2000, 19204, 4697, 999]\n",
      "\n",
      "Tokens Id with special tokens: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n",
      "\n",
      "Decoded Text Output: [CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c49989-f6c6-434f-96e7-628537be41ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['‚ñÅlet', \"'\", 's', '‚ñÅtry', '‚ñÅto', '‚ñÅto', 'ken', 'ize', '!']\n",
      "\n",
      "Tokens Id: [408, 22, 18, 1131, 20, 20, 2853, 2952, 187]\n",
      "\n",
      "Tokens Id with special tokens: [2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 187, 3]\n",
      "\n",
      "Decoded Text Output: [CLS] let's try to tokenize![SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8334035-bab7-4ec7-9fe7-feaf79c97805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['Let', \"'s\", 'ƒ†try', 'ƒ†to', 'ƒ†token', 'ize', '!']\n",
      "\n",
      "Tokens Id: [7939, 18, 860, 7, 19233, 2072, 328]\n",
      "\n",
      "Tokens Id with special tokens: [0, 7939, 18, 860, 7, 19233, 2072, 328, 2]\n",
      "\n",
      "Decoded Text Output: <s>Let's try to tokenize!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef00c7c4-f976-4495-8451-1a48c5d8f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = \"Let's try to tokenize!\"\n",
    "\n",
    "input_ids = tokenizer(inputs)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d4ecbf-2bd0-496f-8fb6-2c8463291dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9eca383-be81-4754-8e28-81b34a74cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eed443-523e-471c-b6f4-13132d663830",
   "metadata": {},
   "source": [
    "## **Padding and Truncation**\n",
    "\n",
    "Batched inputs are often different lengths, so they can‚Äôt be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a **special padding token** to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences.\n",
    "\n",
    "In most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well. However, the API supports more strategies if you need them. The three arguments you need to are: padding, truncation and max_length.\n",
    "\n",
    "### **Pad**\n",
    "\n",
    "Sentences aren‚Äôt always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special *padding token* to shorter sentences.\n",
    "\n",
    "Set the padding parameter to True to pad the shorter sequences in the batch to match the longest sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84eb6ba0-e487-40bf-8647-400f06eed126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 2021, 2054, 2055, 2117, 6350, 1029, 102, 0, 0, 0, 0, 0, 0],\n",
       " [101,\n",
       "  2123,\n",
       "  1005,\n",
       "  1056,\n",
       "  2228,\n",
       "  2002,\n",
       "  4282,\n",
       "  2055,\n",
       "  2117,\n",
       "  6350,\n",
       "  1010,\n",
       "  28315,\n",
       "  1012,\n",
       "  102],\n",
       " [101, 2054, 2055, 5408, 14625, 1029, 102, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "\n",
    "encoded_input = tokenizer(batch_sentences, padding=True)\n",
    "\n",
    "encoded_input[\"input_ids\"]\n",
    "\n",
    "# The first and third sentences are now padded with 0‚Äôs because they are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d6dd4b-e84f-4946-8f23-342639f7d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] don't think he knows about second breakfast, pip. [SEP]\n",
      "[CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for seq in encoded_input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc83d17-cc85-4d92-aba9-2e249e021ad7",
   "metadata": {},
   "source": [
    "## **Fine-Tunning**\n",
    "\n",
    "https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266e326-1588-4286-b414-e32ab3335061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0dd13-753a-43e8-a241-b2248af084a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
